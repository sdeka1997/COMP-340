{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: A study of deep net architectures for the MNIST problem\n",
    "You should use the code from mnist_keras.ipynb to build and evaluate the networks proposed here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ae60ffe7d847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install tensorflow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (7,7)\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Architectural variations on the feedforward network\n",
    "- construct a network with 2 hidden layers and 256 units each.\n",
    "- construct a network with 4 hidden layers and 512, 256, 128, 64 units in successive layers.\n",
    "Compile and train these two networks and see how well they perform on the MNIST task. Describe their accuracies in relation to the model with 2 hidden layers and 512 units each. On the MNIST task, how important is depth, and is a telescoping architecture of the second network a better use of units than the flat architecture of the first one? Explain why. Support your explanation with plots of training performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final accuracies in the form of Network Structure -> Validation Accuracy:\n",
    "\n",
    "256,256 -> 0.9884\n",
    "512,256,128,64 -> 0.989\n",
    "512,512 -> 0.9920\n",
    "256,256,256,256 -> 0.9810\n",
    "256,64 -> 0.9814\n",
    "256 -> 0.9817\n",
    "64 -> 0.9708\n",
    "64,64,64,64 -> 0.9718\n",
    "512 -> .9934\n",
    "10 -> .914\n",
    "\n",
    "\n",
    "I observed the outputs from the analysis and made the conclusion that the depth is not very important based on its impact on performance. One of the 256 layers performed better than the others. The telescoping architecture of the 512,512 was the 2nd best, and was better than the 512,256,128,64, which was also outperformed by the single 512. We realized this probably means the most important thing is nodes per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1a2924b7e554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclass_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_train original shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y_train original shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "class_num = 10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"X_train original shape\", X_train.shape)\n",
    "print(\"y_train original shape\", y_train.shape)\n",
    "\n",
    "img_x, img_y = 28,28\n",
    "X_train = X_train.reshape(60000, img_x*img_y)\n",
    "X_test = X_test.reshape(10000, img_x*img_y)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(\"Training matrix shape\", X_train.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, class_num)\n",
    "Y_test = np_utils.to_categorical(y_test, class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modela = Sequential()\n",
    "\n",
    "modela.add(Dense(256, input_shape=(784,)))\n",
    "modela.add(Activation('relu'))\n",
    "modela.add(Dropout(0.2))\n",
    "\n",
    "modela.add(Dense(256))\n",
    "modela.add(Activation('relu'))\n",
    "modela.add(Dropout(0.2))\n",
    "\n",
    "modela.add(Dense(10))\n",
    "modela.add(Activation('softmax')) \n",
    "\n",
    "modela.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('acc'))\n",
    "\n",
    "history = AccuracyHistory()\n",
    "\n",
    "modela.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=10,\n",
    "          verbose=1,validation_data=(X_test, Y_test),\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelb = Sequential()\n",
    "\n",
    "modelb.add(Dense(512, input_shape=(784,)))\n",
    "modelb.add(Activation('relu'))\n",
    "modelb.add(Dropout(0.2)) \n",
    "\n",
    "modelb.add(Dense(256))\n",
    "modelb.add(Activation('relu'))\n",
    "modelb.add(Dropout(0.2))\n",
    "\n",
    "modelb.add(Dense(128))\n",
    "modelb.add(Activation('relu'))\n",
    "modelb.add(Dropout(0.2))\n",
    "\n",
    "modelb.add(Dense(64))\n",
    "modelb.add(Activation('relu'))\n",
    "modelb.add(Dropout(0.2))\n",
    "\n",
    "modelb.add(Dense(10))\n",
    "modelb.add(Activation('softmax')) \n",
    "\n",
    "\n",
    "modelb.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('acc'))\n",
    "\n",
    "history = AccuracyHistory()\n",
    "\n",
    "\n",
    "modelb.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=10,\n",
    "          verbose=1,validation_data=(X_test, Y_test),\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc = Sequential()\n",
    "\n",
    "modelc.add(Dense(512, input_shape=(784,)))\n",
    "modelc.add(Activation('relu'))\n",
    "modelc.add(Dropout(0.2)) \n",
    "\n",
    "\n",
    "modelc.add(Dense(10))\n",
    "modelc.add(Activation('softmax')) \n",
    "\n",
    "modelc.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('acc'))\n",
    "\n",
    "history = AccuracyHistory()\n",
    "\n",
    "modelc.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=10,\n",
    "          verbose=1,validation_data=(X_test, Y_test),\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Architectural variations on the CNN model\n",
    "- construct a network with \n",
    "     - one conv layer with 32 filters of size 5x5\n",
    "     - a maxpool layer with 2x2 filter and stride of 2,2\n",
    "     - a dense layer of 1000\n",
    "     - a final softmax layer of 10 units\n",
    "     \n",
    "Compare its performance against the previous CNN network. \n",
    "\n",
    "- construct a network with\n",
    "    - one conv layer with 32 filters of size 3X3\n",
    "    - a maxpool layer with 2x2 filter and stride of 2,2\n",
    "    - one conv layer with 64 filters of size 3x3\n",
    "    - a dense layer of 1000\n",
    "    - a final softmax layer of 10 units\n",
    "\n",
    "Compare its performance against the other two CNN networks.\n",
    "\n",
    "What conclusions can you draw about depth of the CNN network and filter size in relation to classification performance? Support your conclusions with plots of training performance, and for extra credit, visualize the filter weights of the first layer in these two architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Results:\n",
    "\n",
    "Conv(32,(3,3)(1,1)):Pool((2,2),(2,2)):Flatten:Dense(1000) -> 0.9875\n",
    "Conv(32,(5,5)(1,1)):Pool((2,2),(2,2)):Flatten:Dense(1000) -> 0.9884\n",
    "Conv(32,(5,5)(1,1)):Pool((2,2),(2,2)):Conv(64,(3,3)):Flatten:Dense(1000) -> 0.9916\n",
    "Conv(32,(5,5)(1,1)):Pool((2,2),(2,2)):Conv(64,(5,5)):Pool((2,2)):Flatten:Dense(1000) -> 0.9920\n",
    "\n",
    "The number of layers is the most important feature of performance. When we move from a single layer 3x3 to 5x5, we see there is not a significant change. We make the most significant improvements in performance when the number of layers is increased. Within layers, there are not significant improvements by increasing filter size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "input_shape = (img_x, img_y, 1)\n",
    "X_train = X_train.reshape(X_train.shape[0], img_x, img_y, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_x, img_y, 1)\n",
    "\n",
    "\n",
    "modela_cnn = Sequential()\n",
    "modela_cnn.add(Conv2D(32, kernel_size=(5, 5), strides=(1,1),activation='relu',input_shape=input_shape))\n",
    "modela_cnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "modela_cnn.add(Flatten())\n",
    "modela_cnn.add(Dense(1000, activation='relu'))\n",
    "modela_cnn.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "modela_cnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "historya_cnn = AccuracyHistory()\n",
    "modela_cnn.fit(X_train, Y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[history1_cnn])\n",
    "\n",
    "\n",
    "scorea = modela_cnn.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', scorea[0])\n",
    "print('Test accuracy:', scorea[1])\n",
    "plt.plot(range(1, 11), historya_cnn.acc)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelb_cnn = Sequential()\n",
    "modelb_cnn.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1),activation='relu',input_shape=input_shape))\n",
    "modelb_cnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "modelb_cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "modelb_cnn.add(Flatten())\n",
    "modelb_cnn.add(Dense(1000, activation='relu'))\n",
    "modelb_cnn.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "modelb_cnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "historyb_cnn = AccuracyHistory()\n",
    "modelb_cnn.fit(X_train, Y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[history2_cnn])\n",
    "\n",
    "\n",
    "\n",
    "scoreb = modelb_cnn.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', scoreb[0])\n",
    "print('Test accuracy:', scoreb[1])\n",
    "plt.plot(range(1, 11), historyb_cnn.acc)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modela_cnn.layers[0].get_weights()\n",
    "\n",
    "modelb2_cnn.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: (optional): A new architectural variation\n",
    "Suggest a new architecture (perhaps a variation on Alexnet, VGGnet, ResNet, ..) for the MNIST problem. Implement that architecture in Keras and plot the training performance and visualize the first layer filter weights. Compare it with the other architectures you have built in Problems 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

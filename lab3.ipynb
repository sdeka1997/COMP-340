{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Yelp reviews for Houston restaurants\n",
    "\n",
    "Welcome to the lab on web scraping. While many people might view working with data (including scraping, parsing, storing, etc.) a necessary evil to get to the \"fun\" stuff (i.e. modeling), I think that if presented in the right way this munging can be quite empowering. Imagine you never had to worry or ask those _what if_ questions about data existing or being accessible... but that you can get it yourself!\n",
    "\n",
    "By the end of this lab hopefully you should look at the wonderful world wide web without fear, comforted by the fact that anything you can see with your  eyes, a computer can see with its  eyes...\n",
    " \n",
    "## Objectives\n",
    "\n",
    "But more concretely, this lab will teach you how to:\n",
    "\n",
    "* HTTP Requests (and lifecycle)\n",
    "* RESTful APIs\n",
    "    * Authentication (OAuth)\n",
    "   \n",
    "(this lab is based on the Practical Data Science Course at CMU, designed by Z Kolter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with APIs\n",
    "\n",
    "Since everyone loves food (presumably), the ultimate end goal of this assignment will be to acquire the data to answer some questions and hypotheses about the restaurant scene in Houston (which we will get to later). We will download __both__ the metadata on restaurants in Houston from the Yelp API and with this metadata, retrieve the comments/reviews and ratings from users on restaurants.\n",
    "\n",
    "But first things first, let's do the \"hello world\" of making web requests with Python to get a sense for how to programmatically access web pages: an (unauthenticated) HTTP GET to download a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic HTTP Requests\n",
    "\n",
    "Fill in the funtion to use `requests` to download and return the raw HTML content of the URL passed in as an argument. As an example, try the following NYT article: [https://www.nytimes.com/2018/04/11/technology/personaltech/i-downloaded-the-information-that-facebook-has-on-me-yikes.html](https://www.nytimes.com/2018/04/11/technology/personaltech/i-downloaded-the-information-that-facebook-has-on-me-yikes.html)\n",
    "\n",
    "> Your function should return a tuple of: (`<status_code>`, `<raw_html>`)\n",
    "\n",
    "```python\n",
    ">>> facebook_article = retrieve_html('https://www.nytimes.com/2018/04/11/technology/personaltech/i-downloaded-the-information-that-facebook-has-on-me-yikes.html')\n",
    ">>> print(facebook_article)\n",
    "(200, u'<!DOCTYPE html>\\n<!--[if (gt IE 9)|!(IE)]> <!--> <html lang=\"en\" class=\"no-js section-magazine...')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def retrieve_html(url):\n",
    "    \"\"\"\n",
    "    Return the raw HTML at the specified URL.\n",
    "\n",
    "    Args:\n",
    "        url (string): \n",
    "\n",
    "    Returns:\n",
    "        status_code (integer):\n",
    "        raw_html (string): the raw HTML content of the response, properly encoded according to the HTTP headers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write solution here (2 lines of code expected)\n",
    "    content = requests.get(url)\n",
    "    return (content.status_code, content.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test retrieve_html function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.nytimes.com/2018/04/11/technology/personaltech/i-downloaded-the-information-that-facebook-has-on-me-yikes.html'\n",
    "(status,nyt_article) = retrieve_html(url) \n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to extract articles using BeautifulSoup\n",
    "\n",
    "Using `BeautifulSoup`, parse the HTML of the retrieved URL to extract the article. \n",
    "Fill in following function stub to get the article out. You will have to look at the structure of the HTML returned by the NYT to determine which elements to extract and how to piece together the items of interest.\n",
    "\n",
    "- to convert the HTML string `article` into a soup object: use `BeautifulSoup(article,\"lxml\")`\n",
    "- to find all paragraph tags in a soup object: use `soup.find_all(\"p\")`\n",
    "- to extract text from a paragraph object: use `p.getText()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_article(article):\n",
    "    \"\"\"\n",
    "    Input: html_string\n",
    "    Output: all paragraphs in the parsed html_string\n",
    "    \"\"\"\n",
    "    # 5 lines of code expected.\n",
    "    art = BeautifulSoup(article,\"lxml\")\n",
    "    paragraphs = art.find('article').find_all(\"p\")\n",
    "    text = []  \n",
    "    for para in paragraphs:\n",
    "        text.append(para.getText())        \n",
    "    return ' '.join(text[4:-5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test extract_article function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When I downloaded a copy of my Facebook data last week, I didn’t expect to see much. My profile is sparse, I rarely post anything on the site, and I seldom click on ads. (I’m what some call a Facebook “lurker.”) But when I opened my file, it was like opening Pandora’s box. With a few clicks, I learned that about 500 advertisers — many that I had never heard of, like Bad Dad, a motorcycle parts store, and Space Jesus, an electronica band — had my contact information, which could include my email address, phone number and full name. Facebook also had my entire phone book, including the number to ring my apartment buzzer. The social network had even kept a permanent record of the roughly 100 people I had deleted from my friends list over the last 14 years, including my exes. There was so much that Facebook knew about me — more than I wanted to know. But after looking at the totality of what the Silicon Valley company had obtained about yours truly, I decided to try to better understand how and why my data was collected and stored. I also sought to find out how much of my data could be removed. How Facebook collects and treats personal information was central this week when Mark Zuckerberg, the company’s chief executive,  answered questions in Congress about data privacy and his responsibilities to users. During his testimony, Mr. Zuckerberg repeatedly said Facebook has a tool for downloading your data that “allows people to see and take out all the information they’ve put into Facebook.” (Those who want to download their own Facebook data can use this link.) But that’s an overstatement. Most basic information, like my birthday, could not be deleted. More important, the pieces of data that I found objectionable, like the record of people I had unfriended, could not be removed from Facebook, either. “They don’t delete anything, and that’s a general policy,” said Gabriel Weinberg, the founder of DuckDuckGo, which offers internet privacy tools. He added that data was kept around to eventually help brands serve targeted ads. Beth Gautier, a Facebook spokeswoman, put it this way: “When you delete something, we remove it so it’s not visible or accessible on Facebook.” She added: “You can also delete your account whenever you want. It may take up to 90 days to delete all backups of data on our servers.” Digging through your Facebook files is an exercise I highly recommend if you care about how your personal information is stored and used. Here’s what I learned. A history of the steps the company took to become an advertising giant. When you download a copy of your Facebook data, you will see a folder containing multiple subfolders and files. The most important one is the “index” file, which is essentially a raw data set of your Facebook account, where you can click through your profile, friends list, timeline and messages, among other features. One surprising part of my index file was a section called Contact Info. This contained the 764 names and phone numbers of everyone in my iPhone’s address book. Upon closer inspection, it turned out that Facebook had stored my entire phone book because I had uploaded it when setting up Facebook’s messaging app, Messenger. This was unsettling. I had hoped Messenger would use my contacts list to find others who were also using the app so that I could connect with them easily — and hold on to the relevant contact information only for the people who were on Messenger. Yet Facebook kept the entire list, including the phone numbers for my car mechanic, my apartment door buzzer and a pizzeria. This felt unnecessary, though Facebook holds on to your phone book partly to keep it synchronized with your contacts list on Messenger and to help find people who newly sign up for the messaging service. I opted to turn off synchronizing and deleted all my phone book entries. My Facebook data also revealed how little the social network forgets. For instance, in addition to recording the exact date I signed up for Facebook in 2004, there was a record of when I deactivated Facebook in October 2010, only to reactivate it four days later — something I barely remember doing. Facebook also kept a history of each time I opened Facebook over the last two years, including which device and web browser I used. On some days, it even logged my locations, like when I was at a hospital two years ago or when I visited Tokyo last year. Facebook keeps a log of this data as a security measure to flag suspicious logins from unknown devices or locations, similar to how banks send a fraud alert when your credit card number is used in a suspicious location. This practice seemed reasonable, so I didn’t try to purge this information. But what bothered me was the data that I had explicitly deleted but that lingered in plain sight. On my friends list, Facebook had a record of “Removed Friends,” a dossier of the 112 people I had removed along with the date I clicked the “Unfriend” button. Why should Facebook remember the people I’ve cut off from my life? Facebook’s explanation was dissatisfying. The company said it might use my list of deleted friends so that those people did not appear in my feed with the feature “On This Day,” which resurfaces memories from years past to help people reminisce. I’d rather have the option to delete the list of deleted friends for good. What Facebook retained about me isn’t remotely as creepy as the sheer number of advertisers that have my information in their databases. I found this out when I clicked on the Ads section in my Facebook file, which loaded a history of the dozen ads I had clicked on while browsing the social network. Lower down, there was a section titled “Advertisers with your contact info,” followed by a list of roughly 500 brands, the overwhelming majority of which I had never interacted with. Some brands sounded obscure and sketchy — one was called “Microphone Check,” which turned out to be a radio show. Other brands were more familiar, like Victoria’s Secret Pink, Good Eggs or AARP. Facebook said unfamiliar advertisers might appear on the list because they might have obtained my contact information from elsewhere, compiled it into a list of people they wanted to target and uploaded that list into Facebook. Brands can upload their customer lists into a tool called Custom Audiences, which helps them find those same people’s Facebook profiles to serve them ads. Brands can obtain your information in many different ways. Those include: ■ Buying information from a data provider like Acxiom, which has amassed one of the world’s largest commercial databases on consumers. Brands can buy different types of customer data sets from a provider, like contact information for people who belong to a certain demographic, and take that information to Facebook to serve targeted ads, said Michael Priem, chief executive of Modern Impact, an advertising firm in Minneapolis. Last month, Facebook announced that it was limiting its practice of allowing advertisers to target ads using information from third-party data brokers like Acxiom. ■ Using tracking technologies like web cookies and invisible pixels that load in your web browser to collect information about your browsing activities. There are many different trackers on the web, and Facebook offers 10 different trackers to help brands harvest your information, according to Ghostery, which offers privacy tools that block ads and trackers. The advertisers can take some pieces of data that they have collected with trackers and upload them into the Custom Audiences tool to serve ads to you on Facebook. ■ Getting your information in simpler ways, too. Someone you shared information with could share it with another entity. Your credit card loyalty program, for example, could share your information with a hotel chain, and that hotel chain could serve you ads on Facebook. The upshot? Even a Facebook lurker, like myself, who has barely clicked on any digital ads can have personal information exposed to an enormous number of advertisers. This was not entirely surprising, but seeing the list of unfamiliar brands with my contact information in my Facebook file was a dose of reality. I tried to contact some of these advertisers, like Very Important Puppets, a toymaker, to ask them about what they did with my data. They did not respond. Let’s be clear: Facebook is just the tip of the iceberg when it comes to what information tech companies have collected on me. Knowing this, I also downloaded copies of my Google data with a tool called Google Takeout. The data sets were exponentially larger than my Facebook data. For my personal email account alone, Google’s archive of my data measured eight gigabytes, enough to hold about 2,000 hours of music. By comparison, my Facebook data was about 650 megabytes, the equivalent of about 160 hours of music. Here was the biggest surprise in what Google collected on me: In a folder labeled Ads, Google kept a history of many news articles I had read, like a Newsweek story about Apple employees walking into glass walls and a New York Times story about the editor of our Modern Love column. I didn’t click on ads for either of these stories, but the search giant logged them because the sites had loaded ads served by Google. In another folder, labeled Android, Google had a record of apps I had opened on an Android phone since 2015, along with the date and time. This felt like an extraordinary level of detail. Google did not immediately respond to a request for comment. On a brighter note, I downloaded an archive of my LinkedIn data. The data set was less than half a megabyte and contained exactly what I had expected: spreadsheets of my LinkedIn contacts and information I had added to my profile. Yet that offered little solace. Be warned: Once you see the vast amount of data that has been collected about you, you won’t be able to unsee it.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_article(nyt_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to APIs\n",
    "\n",
    "Now while this example might have been fun, we haven't yet done anything more than we could with a web browser. To really see the power of programmatically making web requests we will need to interact with a API. For the rest of this homework we will be working with the [Yelp API](https://www.yelp.com/developers/documentation/v3/get_started) and Yelp data (for an extensive data dump see their [Academic Dataset Challenge](https://www.yelp.com/dataset_challenge)). The reasons for using the Yelp API are three fold:\n",
    "\n",
    "1. Incredibly rich dataset that combines:\n",
    "    * entity data (users and businesses)\n",
    "    * preferences (i.e. ratings)\n",
    "    * geographic data (business location and check-ins)\n",
    "    * temporal data\n",
    "    * text in the form of reviews\n",
    "    * and even images.\n",
    "2. Well [documented API](https://www.yelp.com/developers/documentation/v3/get_started) with thorough examples.\n",
    "3. Extensive data coverage so that you can find data that you know personally (from your home town/city or account). This will help with understanding and interpreting your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "To access the Yelp API however we will need to go through a few more steps than we did with the first NYT example. Most large web scale companies use a combination of authentication and rate limiting to control access to their data to ensure that everyone using it abides. The first step (even before we make any request) is to setup a Yelp account if you do not have one and get API credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp API Access\n",
    "\n",
    "1. [Create a Yelp account](https://www.yelp.com/signup) (if you do not have one already)\n",
    "2. [Generate API keys](https://www.yelp.com/developers/v3/manage_app) (if you haven't already). You will only need the API Key (not the Client ID or Client Secret) -- more on that later. This step will ask you to create an App. Just make one up, indicate your industry (Education), provide a short description of your app, and then your email, and you are good to go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our accounts setup we can start making requests! There are various authentication schemes that APIs use, listed here in relative order of complexity:\n",
    "\n",
    "* No authentication\n",
    "* [HTTP basic authentication](https://en.wikipedia.org/wiki/Basic_access_authentication)\n",
    "* Cookie based user login\n",
    "* OAuth (v1.0 & v2.0, see this [post](http://stackoverflow.com/questions/4113934/how-is-oauth-2-different-from-oauth-1) explaining the differences)\n",
    "* API keys\n",
    "* Custom Authentication\n",
    "\n",
    "For the NYT example, since it is a publicly visible page we did not need to authenticate. HTTP basic authentication isn't too common for consumer sites/applications that have the concept of user accounts (like Facebook, LinkedIn, Twitter, etc.) but is simple to setup quickly and you often encounter it on with individual password protected pages/sites. I'm sure you have seen this before somewhere:\n",
    "\n",
    "![http-basic](http://i.stack.imgur.com/QnUZW.png)\n",
    "\n",
    "Cookie based user login is what the majority of services use when you login with a browser (i.e. username and password). Once you sign in to a service like Facebook, the response stores a cookie in your browser to remember that you have logged in (HTTP is stateless). Each subsequent request to the same domain (i.e. any page on `facebook.com`) also sends the cookie that contains the authentication information to remind Facebook's servers that you have already logged in.\n",
    "\n",
    "Many REST APIs however use OAuth (authentication using tokens) which can be thought of a programmatic way to \"login\" _another_ user. Using tokens, a user (or application) only needs to send the login credentials once in the initial authentication and as a response from the server gets a special signed token. This signed token is then sent in future requests to the server (in place of the user credentials).\n",
    "\n",
    "A similar concept common used by many APIs is to assign API Keys to each client that needs access to server resources. The client must then pass the API Key along with _every_ request it makes to the API to authenticate. This is because the server is typically relatively stateless and does not maintain a session between subsequent calls from the same client. Most APIs (including Yelp) allow you to pass the API Key via a special HTTP Header: \"Authorization: Bearer <API_KEY>\". Check out the [docs](https://www.yelp.com/developers/documentation/v3/authentication) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Authenticated HTTP Request with the Yelp API\n",
    "\n",
    "First, store your Yelp credentials in a local file (kept out of version control) which you can read in to authenticate with the API. This file can be any format/structure since you will fill in the function stub below.\n",
    "\n",
    "For example, you may want to store your key in a file called `api_key.txt`:\n",
    "\n",
    "You can then read from the file using:\n",
    "```python\n",
    "def read_api_key(file):\n",
    "    f = open(file,'r')\n",
    "    api_key = f.read().replace('\\n','')\n",
    "    f.close()\n",
    "    return api_key\n",
    "```\n",
    "\n",
    "**KEEP THE API KEY FILE PRIVATE AND OUT OF VERSION CONTROL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_api_key(file):\n",
    "    f = open(file,'r')\n",
    "    api_key = f.read().replace('\\n','')\n",
    "    f.close()\n",
    "    return api_key\n",
    "\n",
    "api_key = read_api_key('api_key.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Yelp API, fill in the following function stub to make an authenticated request to the [search](https://www.yelp.com/developers/documentation/v3/business_search) endpoint.\n",
    "\n",
    "> As a test, search for Indian restaurants  in Houston. You should find 254 total depending on when you search (but this will actually differ from the number of actual Business objects returned... more on this in the next section)\n",
    "\n",
    "When writing the python request, you'll need to pass in a custom header as well as a parameter dictionary. See \n",
    "https://github.com/Yelp/yelp-fusion/blob/master/fusion/python/sample.py\n",
    "\n",
    "```python\n",
    ">>> api_key = read_api_key('api_key.txt')\n",
    ">>> num_records, data = yelp_search(api_key, 'Indian','Houston, TX')\n",
    ">>> print(num_records)\n",
    "254\n",
    ">>> for x in data: \n",
    "print x['name']\n",
    "Surya India\n",
    "Tarka Indian Kitchen\n",
    "Govinda's Vegetarian Cuisine\n",
    "Indika\n",
    "Cowboys & Indians Tex-In Kitchen\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def yelp_search(api_key, query, location,offset=0):\n",
    "    \"\"\"\n",
    "    Make an authenticated request to the Yelp API.\n",
    "\n",
    "    Args:\n",
    "        query (string): Search term\n",
    "\n",
    "    Returns:\n",
    "        total (integer): total number of businesses on Yelp corresponding to the query\n",
    "        businesses (list): list of dicts representing each business\n",
    "    \"\"\"\n",
    "    url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "    headers = {\"Authorization\": \"Bearer %s\" % api_key}\n",
    "    url_params = {\n",
    "        'term': query.replace(' ', '+'),\n",
    "        'location': location.replace(' ', '+')\n",
    "    }\n",
    "    response = requests.request('GET', url, headers=headers, params=url_params)\n",
    "    data = json.loads(response.content)\n",
    "    num = data['total']\n",
    "    objects = data['businesses']\n",
    "    \n",
    "    return (num, objects)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test yelp_search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n",
      "Surya India\n",
      "Tarka Indian Kitchen\n",
      "Govinda's Vegetarian Cuisine\n",
      "Sai Bhog\n",
      "India's Restaurant\n",
      "Indika\n",
      "Desi Kitchen\n",
      "Cowboys & Indians Tex-In Kitchen\n",
      "Pondicheri\n",
      "Maharaja Bhog\n",
      "Sangam Chettinad Indian Cuisine\n",
      "Kiran's\n",
      "Mayuri Express\n",
      "Shiva Indian Restaurant\n",
      "Tandoori Hut\n",
      "Khyber North Indian Grill\n",
      "Deli Deluxe\n",
      "Nirvana Indian Restaurant\n",
      "Aga's Restaurant & Catering\n",
      "Shri Balaji Bhavan\n"
     ]
    }
   ],
   "source": [
    "#print(yelp_search(read_api_key('api_key.txt'),'Indian','Houston, TX'))\n",
    "num_records, data = yelp_search(api_key, 'Indian','Houston, TX')\n",
    "print(num_records)\n",
    "for x in data: \n",
    "    print (x['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of \"Hello World\" of the Yelp API\n",
    "Now that we have completed the \"hello world\" of working with the Yelp API, we are ready to really fly! The next lab will have a bit less direction since there are a variety of ways to retrieve the requested information but you should have all the component knowledge at this point to work with the API. Yelp being a fairly general platform actually has many more business than just restaurants, but by using the flexibility of the API we can ask it to only return the restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
